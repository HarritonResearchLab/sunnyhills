{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(path:str):\n",
    "    '''\n",
    "    returns:\n",
    "    filtered_df: the edited dataframe  \n",
    "    '''\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # drop initials nans\n",
    "    df = df.dropna(subset=['pl_orbper','sy_pnum','pl_controv_flag'])\n",
    "\n",
    "    df = df.where(np.logical_and(df['pl_orbper']>.5,df['pl_orbper']<15))\n",
    "    df = df.where(df['pl_controv_flag']==0)\n",
    "    df = df.where(df['sy_pnum']==1)\n",
    "\n",
    "    # filter out the data where the given fields did not meet the conditions\n",
    "    filtered_df = df.dropna(subset=['pl_orbper','pl_controv_flag','sy_pnum'])\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def remove_flares(time, flux, flux_err=np.array([]), sigma:int=3): \n",
    "    ''' \n",
    "    Args:\n",
    "        time: array-like (ideally Pandas.series) object of time values \n",
    "        flux: array-like (ideally Pandas.series) object of flux values\n",
    "        flux_err: optional array-like (ideally Pandas.series) object of flux err values\n",
    "    Returns: \n",
    "        two tuples (or three if an array was passed into flux_err); first tuple gives arrays that have been \"cleaned\" of flairs, and second gives the removed values. Array order in each tuple is time, flux, (flux_err)\n",
    "    Additional Info: \n",
    "        Idea was taken from https://iopscience.iop.org/article/10.3847/1538-3881/ab5d3a\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "    if isinstance(time, pd.Series) and isinstance(flux, pd.Series): \n",
    "        pass\n",
    "    else: \n",
    "        time = pd.Series(time)\n",
    "        flux = pd.Series(flux)\n",
    "        if len(flux_err)>0: \n",
    "            flux_err = pd.Series(flux_err)\n",
    "            removed_flux_err = np.array([])\n",
    "\n",
    "    removed_time, removed_flux = (np.array([]), np.array([]))\n",
    "\n",
    "    continue_global = True \n",
    "    while continue_global:\n",
    "        length = len(flux)\n",
    "\n",
    "        # # We use three copies and extract the flares from the middle to prevent edge effects --> BREDALLL! LOL\n",
    "        three_flux = pd.concat((flux, flux, flux))\n",
    "        global_rolling = three_flux.rolling(1024, center=False) \n",
    "        global_medians = global_rolling.median()[length:2*length]\n",
    "        global_stds = global_rolling.std()[length:2*length]\n",
    "        cutoffs = global_medians+sigma*global_stds\n",
    "\n",
    "        remove_indices = []\n",
    "        for i in flux.index: \n",
    "            if flux[i]>cutoffs[i] and global_medians[i]!=np.nan: \n",
    "                remove_indices.append(i)\n",
    "\n",
    "        if len(remove_indices)==0: \n",
    "            continue_global = False \n",
    "\n",
    "        else:     \n",
    "            removed_time = np.concatenate((removed_time, time[remove_indices])) \n",
    "            removed_flux = np.concatenate((removed_flux, flux[remove_indices])) \n",
    "\n",
    "            time = time.drop(remove_indices)\n",
    "            flux = flux.drop(remove_indices)\n",
    "\n",
    "            if len(flux_err)>0: \n",
    "                removed_flux_err = np.concatenate((removed_flux_err, flux_err[remove_indices]))\n",
    "                flux_err = flux_err.drop(remove_indices)\n",
    "            \n",
    "    continue_local = True \n",
    "    while continue_local: \n",
    "        local_rolling = flux.rolling(128, center=True) \n",
    "        local_medians = local_rolling.median() \n",
    "        local_stds = local_rolling.std() \n",
    "        cutoffs = local_medians+sigma*local_stds \n",
    "\n",
    "        remove_indices = [] \n",
    "        for i in flux.index: \n",
    "            if flux[i]>cutoffs[i] and local_medians[i]!=np.nan: \n",
    "                remove_indices.append(i) \n",
    "\n",
    "        if len(remove_indices)==0: \n",
    "            continue_local = False \n",
    "\n",
    "        else:     \n",
    "            removed_time = np.concatenate((removed_time, time[remove_indices]))\n",
    "            removed_flux = np.concatenate((removed_flux, flux[remove_indices]))\n",
    "\n",
    "            time = time.drop(remove_indices)\n",
    "            flux = flux.drop(remove_indices)\n",
    "\n",
    "            if len(flux_err)>0: \n",
    "                removed_flux_err = np.concatenate((removed_flux_err, flux_err[remove_indices]))\n",
    "                flux_err = flux_err.drop(remove_indices)\n",
    "\n",
    "    if len(flux_err)>0: \n",
    "        return (time.to_numpy(), flux.to_numpy(), flux_err.to_numpy()), (removed_time, removed_flux, removed_flux_err)\n",
    "    else: \n",
    "        return (time.to_numpy(), flux.to_numpy()), (removed_time, removed_flux)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cSpell:ignore lightkurve, biweight, cval, dtrdict, detrended\n",
    "# cSpell: disable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## DATA PROCESSING RELATED ##\n",
    "\n",
    "def download(\n",
    "    ticstr: str, \n",
    "    outdir: str = 'none', \n",
    "    logdir: str = 'none'): \n",
    "    \n",
    "    ''' \n",
    "    Args:\n",
    "        outdir: directory where lightcurves will be saved. If not set, data will not be saved. \n",
    "        ticstr: e.g., 'TIC 441420236'\n",
    "        logdir: directory for log file\n",
    "    Returns: \n",
    "        raw_list: list of light curve ojects that mean criteria but have not been processed (i.e. not detrended, normalized, or sigma-clipped) \n",
    "        data_found: boolean variable telling you if data were found or not \n",
    "    '''\n",
    "\n",
    "    import numpy as np \n",
    "    import lightkurve as lk \n",
    "    import os \n",
    "    import pickle \n",
    "    import warnings\n",
    "\n",
    "    # get the light curve\n",
    "    data_found = False\n",
    "    \n",
    "    warnings.warn('WARNING: THIS SHOULD NO LONGER BE USED BY ITSELF! NEEDS TO BE FIXED...USE download_preprocess INSTEAD')\n",
    "\n",
    "    '''\n",
    "    lcset = lk.search_lightcurve(ticstr) # otherwise it'll fail for TIC IDs without 120 second cadence data.\n",
    "    if len(lcset) > 0:\n",
    "        lcc = lcset[(lcset.author=='SPOC') & (lcset.exptime.value==120)].download_all()\n",
    "        data_found = True\n",
    "    '''\n",
    "\n",
    "    lcc = lk.search_lightcurve(ticstr.replace('_', ' ')).download_all() # FIX THIS! \n",
    "    if lcc != None: \n",
    "        data_found = True \n",
    "\n",
    "    # select only the two-minute cadence SPOC-reduced data; convert to a list.\n",
    "    # note that this conversion approach works for any LightCurveCollection\n",
    "    # returned by lightkurve -- no need to hand-pick the right ones.  the exact\n",
    "    # condition below says \"if the interval is between 119 and 121 seconds,\n",
    "    # take it\".\n",
    "\n",
    "    if data_found: \n",
    "        raw_list = [_l for _l in lcc\n",
    "                if\n",
    "                _l.meta['ORIGIN']=='NASA/Ames'\n",
    "                and\n",
    "                np.isclose(\n",
    "                    120,\n",
    "                    np.nanmedian(np.diff(_l.remove_outliers().time.value))*24*60*60,\n",
    "                    atol=1\n",
    "                )\n",
    "        ]\n",
    "\n",
    "        raw_list = [_l for _l in raw_list if _l.meta['FLUX_ORIGIN']=='pdcsap_flux']\n",
    "        \n",
    "        if len(raw_list) == 0: \n",
    "            data_found = False \n",
    "\n",
    "        if data_found: \n",
    "            new_raw_list = []\n",
    "\n",
    "            for lc in raw_list: \n",
    "                time = lc.time.value\n",
    "                flux = lc.flux.value\n",
    "\n",
    "                nan_mask = np.isnan(flux)\n",
    "\n",
    "                time = time[~nan_mask]\n",
    "                flux = np.array(flux[~nan_mask], dtype=float) \n",
    "\n",
    "                qual = lc.quality.value\n",
    "\n",
    "                # remove non-zero quality flags\n",
    "                sel = (qual[~nan_mask] == 0)\n",
    "\n",
    "                time = time[sel]\n",
    "                flux = flux[sel]\n",
    "\n",
    "                # normalize around 1\n",
    "                flux /= np.nanmedian(flux)\n",
    "\n",
    "                new_raw_list.append({'time':time, 'flux':flux})\n",
    "\n",
    "            raw_list = new_raw_list \n",
    "    \n",
    "    if data_found: \n",
    "        if outdir != 'none': \n",
    "            joined = {'raw_list':raw_list}\n",
    "            outfile = outdir + '/' + ticstr.replace(' ', '_') + '_raw_lc.pickle'\n",
    "            \n",
    "            with open(outfile, 'wb') as handle:\n",
    "                pickle.dump(joined, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            warnings.warn(\"FIX THIS TO CSV!!!\")\n",
    "            warnings.warn('idea for the future: just only use download and preprocess together')\n",
    "\n",
    "    if not data_found: \n",
    "        raw_list = None\n",
    "\n",
    "    '''\n",
    "    if logdir != 'none': \n",
    "            logfile = logdir + '/' + ticstr.replace(\" \",'_')+'_log.pickle'\n",
    "\n",
    "            logfile = convert_to_absolute_path(logfile)\n",
    "            if os.path.exists(logfile): \n",
    "                with open(logfile, 'rb') as f: \n",
    "                    content = pickle.load(f)\n",
    "                    content['sectors']=len(raw_list)\n",
    "\n",
    "            else: \n",
    "                content = {'sectors':len(raw_list)}\n",
    "\n",
    "            with open(logfile, 'wb') as f: \n",
    "                    pickle.dump(content, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    '''\n",
    "    return raw_list, data_found\n",
    "\n",
    "def preprocess(\n",
    "    raw_list: list,\n",
    "    ticstr: str = '',\n",
    "    outdir: str = \"none\", \n",
    "    dtrdict: dict = {'method':'biweight',\n",
    "                     'window_length':0.5,\n",
    "                     'cval':5.0,\n",
    "                     \"break_tolerance\":1.0}, \n",
    "    lower_sigma: int = 10\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        raw_list: raw list of light curves; see download() \n",
    "        outdir: directory where lightcurves will be saved. If not set, data will not be saved.  --> FIX!\n",
    "        ticstr: e.g., 'TIC 441420236'.  \n",
    "        dtrdict: dictionary with keys \"window_length\", \"method\", \"cval\",\n",
    "                 \"break_tolerance\", or anything else needed by wotan's `flatten`\n",
    "                 call.  These are documented at\n",
    "                 https://wotan.readthedocs.io/en/latest/Usage.html \n",
    "        lower_sigma: sigma value for \"lower\" (non-Gunther) level sigma clip. Default is 10.  \n",
    "    Returns: \n",
    "        lc_list: list of light curve ojects that have met all criteria, been removed of outliers, normalized, and flattened. \n",
    "        trend_list: list of light curve objects with x = time, y = trend\n",
    "        raw_list: list of the raw light curve objects \n",
    "    \"\"\"\n",
    "\n",
    "    import numpy as np \n",
    "    import wotan \n",
    "    import pandas as pd\n",
    "    import warnings\n",
    "\n",
    "    cleaned_time = np.array([])\n",
    "    detrended_flux = np.array([])\n",
    "    trend_time = np.array([])\n",
    "    trend_flux = np.array([])\n",
    "    raw_time = np.array([])\n",
    "    raw_flux = np.array([])\n",
    "\n",
    "    for lc in raw_list:\n",
    "\n",
    "        time = lc['time']\n",
    "        flux = lc['flux']\n",
    "\n",
    "        raw_time = np.concatenate((raw_time, time))\n",
    "        raw_flux = np.concatenate((raw_flux, flux))\n",
    "\n",
    "        def old_detrend_method(time, flux): \n",
    "\n",
    "            # remove outliers before local window detrending-- wotan does this before detrend, and sigma clip after detrend \n",
    "            clipped_flux = wotan.slide_clip(time, flux, window_length=0.5, low=3,\n",
    "                                    high=2, method='mad', center='median')\n",
    "\n",
    "            clipped_mask = ~np.isnan(clipped_flux)\n",
    "\n",
    "            clipped_time = time[clipped_mask]\n",
    "            clipped_flux = clipped_flux[clipped_mask]\n",
    "\n",
    "            # see https://wotan.readthedocs.io/en/latest/Usage.html for other\n",
    "            # possible options.\n",
    "            flat_flux, trend_flux = wotan.flatten(\n",
    "                clipped_time, clipped_flux, return_trend=True,\n",
    "                method=dtrdict['method'],\n",
    "                break_tolerance=dtrdict['break_tolerance'],\n",
    "                window_length=dtrdict['window_length'],\n",
    "                cval=dtrdict['cval']\n",
    "            )\n",
    "\n",
    "            flat_mean, flat_sigma = (np.nanmean(flat_flux), np.nanstd(flat_flux))\n",
    "\n",
    "            #_, *bounds = sigma_clip(flat_flux, sigma_lower=10, sigma_upper=1, maxiters=1, masked=False, return_bounds=True) # okay flex LOL\n",
    "\n",
    "            bounds = [flat_mean-sigma_bounds[0]*flat_sigma, flat_mean+sigma_bounds[1]*flat_sigma] # save these bounds to log file? \n",
    "\n",
    "            flat_mask = np.logical_and(flat_flux<bounds[1], flat_flux>bounds[0])\n",
    "\n",
    "            flat_time = clipped_time[flat_mask]\n",
    "            flat_flux = flat_flux[flat_mask]\n",
    "\n",
    "            lc_times = np.concatenate((lc_times, flat_time))\n",
    "            lc_fluxes = np.concatenate((lc_fluxes, flat_flux))\n",
    "\n",
    "            trend_times = np.concatenate((trend_times, clipped_time))\n",
    "            trend_fluxes = np.concatenate((trend_fluxes, trend_flux))\n",
    "\n",
    "            raw_time = np.concatenate((raw_times, time))\n",
    "            raw_fluxe = np.concatenate((raw_fluxes, flux))\n",
    "             \n",
    "        # remove stuff below \n",
    "\n",
    "        continue_lower_cut = True \n",
    "        while continue_lower_cut: \n",
    "            below_lower_cut = np.where(flux<(np.median(flux)-lower_sigma*np.std(flux)))[0]\n",
    "            if len(below_lower_cut)>0: \n",
    "                time = np.delete(time, below_lower_cut)\n",
    "                flux = np.delete(flux, below_lower_cut)\n",
    "\n",
    "            else: \n",
    "                continue_lower_cut=False\n",
    "\n",
    "        (cleaned_time_temp, cleaned_flux_temp), (_, _) = remove_flares(time, flux)\n",
    "\n",
    "        detrended_flux_temp, trend_flux_temp = wotan.flatten(\n",
    "            cleaned_time_temp, cleaned_flux_temp, return_trend=True,\n",
    "            method=dtrdict['method'],\n",
    "            break_tolerance=dtrdict['break_tolerance'],\n",
    "            window_length=dtrdict['window_length'],\n",
    "            cval=dtrdict['cval']\n",
    "        )\n",
    "\n",
    "        (cleaned_time_temp, detrended_flux_temp, trend_flux_temp), (_, _, _) = remove_flares(cleaned_time_temp, detrended_flux_temp, trend_flux_temp)\n",
    "\n",
    "        cleaned_time = np.concatenate((cleaned_time, cleaned_time_temp))\n",
    "        detrended_flux = np.concatenate((detrended_flux, detrended_flux_temp))\n",
    "        trend_time = np.concatenate((trend_time, cleaned_time_temp))\n",
    "        trend_flux = np.concatenate((trend_flux, trend_flux_temp))\n",
    "\n",
    "    if outdir != 'none': \n",
    "        if outdir[-1]!='/':\n",
    "            outdir+='/'\n",
    "        \n",
    "        outfile = outdir+ticstr.replace(' ','_')+'.csv'\n",
    "        \n",
    "        cols = [cleaned_time, detrended_flux, trend_time, trend_flux, raw_time, raw_flux]\n",
    "        cols = [pd.Series(i) for i in cols]\n",
    "\n",
    "        col_names = ['cleaned_time', 'detrended_flux', 'trend_time', 'trend_flux', 'raw_time', 'raw_flux']\n",
    "    \n",
    "        dictionary = {}\n",
    "        for i in range(len(cols)):\n",
    "            dictionary.update({col_names[i]:cols[i]})\n",
    "\n",
    "        out_df = pd.DataFrame(dictionary)\n",
    "\n",
    "        out_df.to_csv(outfile, index=False)\n",
    "\n",
    "        # cleaned as in flares have been removed \n",
    "        # detrended has had flares removed and trend removed as well  \n",
    "\n",
    "    return (cleaned_time, detrended_flux), (cleaned_time, trend_flux), (raw_time, raw_flux) \n",
    "\n",
    "def download_and_preprocess(\n",
    "    ticstr: str = '',\n",
    "    outdir: str = 'none', \n",
    "    logdir: str = 'none', \n",
    "    dtrdict: dict = {'method':'biweight',\n",
    "                     'window_length':0.5,\n",
    "                     'cval':5.0,\n",
    "                     \"break_tolerance\":1.0}, \n",
    "    sigma_bounds: list = [10, 2]\n",
    "    ): \n",
    "    \n",
    "    '''\n",
    "    Args: \n",
    "        ticstr: e.g. 'TIC 441420236'\n",
    "        outdir: dir to save light curve to. default is none\n",
    "        logdir: dir to save log file to. default is none\n",
    "        dtrdict: detrending dictionary \n",
    "        sigma_bounds: bounds for sigma clipping \n",
    "        \n",
    "    Returns: \n",
    "        lc_list: list of light curve ojects that have met all criteria, been removed of outliers, normalized, and flattened. \n",
    "        trend_list: list of light curve objects with x = time, y = trend\n",
    "        raw_list: list of the raw light curve objects \n",
    "        data_found: if data was not found during download, returns tuple of None objects\n",
    "    '''\n",
    "\n",
    "    import numpy as np\n",
    "    import warnings \n",
    "\n",
    "    raw_list, data_found = download(ticstr=ticstr, logdir=logdir) \n",
    "\n",
    "    if data_found: \n",
    "        stitched_lc, stitched_trend, stitched_raw = preprocess(raw_list=raw_list, ticstr=ticstr, outdir=outdir, dtrdict=dtrdict)\n",
    "\n",
    "    else: \n",
    "        stitched_lc, stitched_trend, stitched_raw = (None, None, None)\n",
    "    \n",
    "    warnings.warn('need to FIX/reimplement LOGGING! and get rid of download returning stitched lc!')\n",
    "\n",
    "    return stitched_lc, stitched_trend, stitched_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bls_check():\n",
    "    df = filter_data('C:/Users/60002/Documents/GitHub/sunnyhills/personal_epochs/veronica/may/validation/known_TESS_planets.csv')\n",
    "    list = df.sample(n=10)\n",
    "\n",
    "    for tid in list['tic_id']:\n",
    "        lc = download_and_preprocess(tid)\n",
    "        best_params, results, bls_model, in_transit, stats = run_bls(lc[0], lc[1])\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\60002\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:32: UserWarning: WARNING: THIS SHOULD NO LONGER BE USED BY ITSELF! NEEDS TO BE FIXED...USE download_preprocess INSTEAD\n",
      "C:\\Users\\60002\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:304: UserWarning: need to FIX/reimplement LOGGING! and get rid of download returning stitched lc!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.01061873, 1.01279824, 1.01621008, ..., 1.00396972, 1.00613564,\n",
       "       1.01181553])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "483cc6728aea19aea79f46d08e82e5df36712545a012895e9b7c5b35b9f2510c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
