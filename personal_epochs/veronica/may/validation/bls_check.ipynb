{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bls_check.ipynb","provenance":[],"authorship_tag":"ABX9TyPCFCXq+qIy6yYYDW/fzcch"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install lightkurve\n","!pip install wotan"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AtXHCNq9bDgz","executionInfo":{"status":"ok","timestamp":1654223921268,"user_tz":420,"elapsed":6211,"user":{"displayName":"Veronica Diaz","userId":"12949637830139422484"}},"outputId":"ab170d41-35f6-4b9f-8681-178b1cdc2bac"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: lightkurve in /usr/local/lib/python3.7/dist-packages (2.0.11)\n","Requirement already satisfied: beautifulsoup4>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from lightkurve) (4.6.3)\n","Requirement already satisfied: bokeh>=1.0 in /usr/local/lib/python3.7/dist-packages (from lightkurve) (2.3.3)\n","Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from lightkurve) (1.4.1)\n","Requirement already satisfied: matplotlib>=1.5.3 in /usr/local/lib/python3.7/dist-packages (from lightkurve) (3.2.2)\n","Requirement already satisfied: memoization>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from lightkurve) (0.4.0)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from lightkurve) (1.21.6)\n","Requirement already satisfied: tqdm>=4.25.0 in /usr/local/lib/python3.7/dist-packages (from lightkurve) (4.64.0)\n","Requirement already satisfied: fbpca>=1.0 in /usr/local/lib/python3.7/dist-packages (from lightkurve) (1.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.7/dist-packages (from lightkurve) (1.3.5)\n","Requirement already satisfied: astroquery>=0.3.10 in /usr/local/lib/python3.7/dist-packages (from lightkurve) (0.4.6)\n","Requirement already satisfied: uncertainties>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from lightkurve) (3.1.6)\n","Requirement already satisfied: oktopus>=0.1.2 in /usr/local/lib/python3.7/dist-packages (from lightkurve) (0.1.2)\n","Requirement already satisfied: astropy>=4.1 in /usr/local/lib/python3.7/dist-packages (from lightkurve) (4.3.1)\n","Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from lightkurve) (1.0.2)\n","Requirement already satisfied: patsy>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from lightkurve) (0.5.2)\n","Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from lightkurve) (2.23.0)\n","Requirement already satisfied: pyerfa>=1.7.3 in /usr/local/lib/python3.7/dist-packages (from astropy>=4.1->lightkurve) (2.0.0.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from astropy>=4.1->lightkurve) (4.11.4)\n","Requirement already satisfied: keyring>=4.0 in /usr/local/lib/python3.7/dist-packages (from astroquery>=0.3.10->lightkurve) (23.5.1)\n","Requirement already satisfied: html5lib>=0.999 in /usr/local/lib/python3.7/dist-packages (from astroquery>=0.3.10->lightkurve) (1.0.1)\n","Requirement already satisfied: pyvo>=1.1 in /usr/local/lib/python3.7/dist-packages (from astroquery>=0.3.10->lightkurve) (1.2.1)\n","Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0->lightkurve) (5.1.1)\n","Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0->lightkurve) (3.13)\n","Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0->lightkurve) (21.3)\n","Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0->lightkurve) (7.1.2)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0->lightkurve) (4.2.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0->lightkurve) (2.8.2)\n","Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0->lightkurve) (2.11.3)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from html5lib>=0.999->astroquery>=0.3.10->lightkurve) (1.15.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from html5lib>=0.999->astroquery>=0.3.10->lightkurve) (0.5.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh>=1.0->lightkurve) (2.0.1)\n","Requirement already satisfied: jeepney>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from keyring>=4.0->astroquery>=0.3.10->lightkurve) (0.8.0)\n","Requirement already satisfied: SecretStorage>=3.2 in /usr/local/lib/python3.7/dist-packages (from keyring>=4.0->astroquery>=0.3.10->lightkurve) (3.3.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->astropy>=4.1->lightkurve) (3.8.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.3->lightkurve) (3.0.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.3->lightkurve) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.3->lightkurve) (1.4.2)\n","Requirement already satisfied: autograd in /usr/local/lib/python3.7/dist-packages (from oktopus>=0.1.2->lightkurve) (1.4)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.4->lightkurve) (2022.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->lightkurve) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->lightkurve) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->lightkurve) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->lightkurve) (2022.5.18.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.0->lightkurve) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.0->lightkurve) (3.1.0)\n","Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.7/dist-packages (from SecretStorage>=3.2->keyring>=4.0->astroquery>=0.3.10->lightkurve) (37.0.2)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring>=4.0->astroquery>=0.3.10->lightkurve) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring>=4.0->astroquery>=0.3.10->lightkurve) (2.21)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from uncertainties>=3.1.4->lightkurve) (0.16.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: wotan in /usr/local/lib/python3.7/dist-packages (1.10)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from wotan) (1.4.1)\n","Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from wotan) (0.51.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from wotan) (1.21.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->wotan) (57.4.0)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->wotan) (0.34.0)\n"]}]},{"cell_type":"code","source":["import numpy as np\n","def remove_flares(time, flux, flux_err=np.array([]), sigma:int=3): \n","    ''' \n","    Args:\n","        time: array-like (ideally Pandas.series) object of time values \n","        flux: array-like (ideally Pandas.series) object of flux values\n","        flux_err: optional array-like (ideally Pandas.series) object of flux err values\n","    Returns: \n","        two tuples (or three if an array was passed into flux_err); first tuple gives arrays that have been \"cleaned\" of flairs, and second gives the removed values. Array order in each tuple is time, flux, (flux_err)\n","    Additional Info: \n","        Idea was taken from https://iopscience.iop.org/article/10.3847/1538-3881/ab5d3a\n","    '''\n","    import pandas as pd\n","    import numpy as np\n","\n","\n","    if isinstance(time, pd.Series) and isinstance(flux, pd.Series): \n","        pass\n","    else: \n","        time = pd.Series(time)\n","        flux = pd.Series(flux)\n","        if len(flux_err)>0: \n","            flux_err = pd.Series(flux_err)\n","            removed_flux_err = np.array([])\n","\n","    removed_time, removed_flux = (np.array([]), np.array([]))\n","\n","    continue_global = True \n","    while continue_global:\n","        length = len(flux)\n","\n","        # # We use three copies and extract the flares from the middle to prevent edge effects --> BREDALLL! LOL\n","        three_flux = pd.concat((flux, flux, flux))\n","        global_rolling = three_flux.rolling(1024, center=False) \n","        global_medians = global_rolling.median()[length:2*length]\n","        global_stds = global_rolling.std()[length:2*length]\n","        cutoffs = global_medians+sigma*global_stds\n","\n","        remove_indices = []\n","        for i in flux.index: \n","            if flux[i]>cutoffs[i] and global_medians[i]!=np.nan: \n","                remove_indices.append(i)\n","\n","        if len(remove_indices)==0: \n","            continue_global = False \n","\n","        else:     \n","            removed_time = np.concatenate((removed_time, time[remove_indices])) \n","            removed_flux = np.concatenate((removed_flux, flux[remove_indices])) \n","\n","            time = time.drop(remove_indices)\n","            flux = flux.drop(remove_indices)\n","\n","            if len(flux_err)>0: \n","                removed_flux_err = np.concatenate((removed_flux_err, flux_err[remove_indices]))\n","                flux_err = flux_err.drop(remove_indices)\n","            \n","    continue_local = True \n","    while continue_local: \n","        local_rolling = flux.rolling(128, center=True) \n","        local_medians = local_rolling.median() \n","        local_stds = local_rolling.std() \n","        cutoffs = local_medians+sigma*local_stds \n","\n","        remove_indices = [] \n","        for i in flux.index: \n","            if flux[i]>cutoffs[i] and local_medians[i]!=np.nan: \n","                remove_indices.append(i) \n","\n","        if len(remove_indices)==0: \n","            continue_local = False \n","\n","        else:     \n","            removed_time = np.concatenate((removed_time, time[remove_indices]))\n","            removed_flux = np.concatenate((removed_flux, flux[remove_indices]))\n","\n","            time = time.drop(remove_indices)\n","            flux = flux.drop(remove_indices)\n","\n","            if len(flux_err)>0: \n","                removed_flux_err = np.concatenate((removed_flux_err, flux_err[remove_indices]))\n","                flux_err = flux_err.drop(remove_indices)\n","\n","    if len(flux_err)>0: \n","        return (time.to_numpy(), flux.to_numpy(), flux_err.to_numpy()), (removed_time, removed_flux, removed_flux_err)\n","    else: \n","        return (time.to_numpy(), flux.to_numpy()), (removed_time, removed_flux)\n","\n"],"metadata":{"id":"8IRTeiySaRz2","executionInfo":{"status":"ok","timestamp":1654223921268,"user_tz":420,"elapsed":6,"user":{"displayName":"Veronica Diaz","userId":"12949637830139422484"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["def download(\n","    ticstr: str, \n","    outdir: str = 'none', \n","    logdir: str = 'none'): \n","    \n","    ''' \n","    Args:\n","        outdir: directory where lightcurves will be saved. If not set, data will not be saved. \n","        ticstr: e.g., 'TIC 441420236'\n","        logdir: directory for log file\n","    Returns: \n","        raw_list: list of light curve ojects that mean criteria but have not been processed (i.e. not detrended, normalized, or sigma-clipped) \n","        data_found: boolean variable telling you if data were found or not \n","    '''\n","\n","    import numpy as np \n","    import lightkurve as lk \n","    import os \n","    import pickle \n","    import warnings\n","\n","    # get the light curve\n","    data_found = False\n","    \n","    warnings.warn('WARNING: THIS SHOULD NO LONGER BE USED BY ITSELF! NEEDS TO BE FIXED...USE download_preprocess INSTEAD')\n","\n","    '''\n","    lcset = lk.search_lightcurve(ticstr) # otherwise it'll fail for TIC IDs without 120 second cadence data.\n","    if len(lcset) > 0:\n","        lcc = lcset[(lcset.author=='SPOC') & (lcset.exptime.value==120)].download_all()\n","        data_found = True\n","    '''\n","\n","    lcc = lk.search_lightcurve(ticstr.replace('_', ' ')).download_all() # FIX THIS! \n","    if lcc != None: \n","        data_found = True \n","\n","    # select only the two-minute cadence SPOC-reduced data; convert to a list.\n","    # note that this conversion approach works for any LightCurveCollection\n","    # returned by lightkurve -- no need to hand-pick the right ones.  the exact\n","    # condition below says \"if the interval is between 119 and 121 seconds,\n","    # take it\".\n","\n","    if data_found: \n","        raw_list = [_l for _l in lcc\n","                if\n","                _l.meta['ORIGIN']=='NASA/Ames'\n","                and\n","                np.isclose(\n","                    120,\n","                    np.nanmedian(np.diff(_l.remove_outliers().time.value))*24*60*60,\n","                    atol=1\n","                )\n","        ]\n","\n","        raw_list = [_l for _l in raw_list if _l.meta['FLUX_ORIGIN']=='pdcsap_flux']\n","        \n","        if len(raw_list) == 0: \n","            data_found = False \n","\n","        if data_found: \n","            new_raw_list = []\n","\n","            for lc in raw_list: \n","                time = lc.time.value\n","                flux = lc.flux.value\n","\n","                nan_mask = np.isnan(flux)\n","\n","                time = time[~nan_mask]\n","                flux = np.array(flux[~nan_mask], dtype=float) \n","\n","                qual = lc.quality.value\n","\n","                # remove non-zero quality flags\n","                sel = (qual[~nan_mask] == 0)\n","\n","                time = time[sel]\n","                flux = flux[sel]\n","\n","                # normalize around 1\n","                flux /= np.nanmedian(flux)\n","\n","                new_raw_list.append({'time':time, 'flux':flux})\n","\n","            raw_list = new_raw_list \n","    \n","    if data_found: \n","        if outdir != 'none': \n","            joined = {'raw_list':raw_list}\n","            outfile = outdir + '/' + ticstr.replace(' ', '_') + '_raw_lc.pickle'\n","            \n","            with open(outfile, 'wb') as handle:\n","                pickle.dump(joined, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","            warnings.warn(\"FIX THIS TO CSV!!!\")\n","            warnings.warn('idea for the future: just only use download and preprocess together')\n","\n","    if not data_found: \n","        raw_list = None\n","\n","    '''\n","    if logdir != 'none': \n","            logfile = logdir + '/' + ticstr.replace(\" \",'_')+'_log.pickle'\n","\n","            logfile = convert_to_absolute_path(logfile)\n","            if os.path.exists(logfile): \n","                with open(logfile, 'rb') as f: \n","                    content = pickle.load(f)\n","                    content['sectors']=len(raw_list)\n","\n","            else: \n","                content = {'sectors':len(raw_list)}\n","\n","            with open(logfile, 'wb') as f: \n","                    pickle.dump(content, f, protocol=pickle.HIGHEST_PROTOCOL)\n","    '''\n","    return raw_list, data_found\n","\n","def preprocess(\n","    raw_list: list,\n","    ticstr: str = '',\n","    outdir: str = \"none\", \n","    dtrdict: dict = {'method':'biweight',\n","                     'window_length':0.5,\n","                     'cval':5.0,\n","                     \"break_tolerance\":1.0}, \n","    lower_sigma: int = 10\n","    ):\n","    \"\"\"\n","    Args:\n","        raw_list: raw list of light curves; see download() \n","        outdir: directory where lightcurves will be saved. If not set, data will not be saved.  --> FIX!\n","        ticstr: e.g., 'TIC 441420236'.  \n","        dtrdict: dictionary with keys \"window_length\", \"method\", \"cval\",\n","                 \"break_tolerance\", or anything else needed by wotan's `flatten`\n","                 call.  These are documented at\n","                 https://wotan.readthedocs.io/en/latest/Usage.html \n","        lower_sigma: sigma value for \"lower\" (non-Gunther) level sigma clip. Default is 10.  \n","    Returns: \n","        lc_list: list of light curve ojects that have met all criteria, been removed of outliers, normalized, and flattened. \n","        trend_list: list of light curve objects with x = time, y = trend\n","        raw_list: list of the raw light curve objects \n","    \"\"\"\n","\n","    import numpy as np \n","    import wotan \n","    import pandas as pd\n","    import warnings\n","\n","    cleaned_time = np.array([])\n","    detrended_flux = np.array([])\n","    trend_time = np.array([])\n","    trend_flux = np.array([])\n","    raw_time = np.array([])\n","    raw_flux = np.array([])\n","\n","    for lc in raw_list:\n","\n","        time = lc['time']\n","        flux = lc['flux']\n","\n","        raw_time = np.concatenate((raw_time, time))\n","        raw_flux = np.concatenate((raw_flux, flux))\n","\n","        def old_detrend_method(time, flux): \n","\n","            # remove outliers before local window detrending-- wotan does this before detrend, and sigma clip after detrend \n","            clipped_flux = wotan.slide_clip(time, flux, window_length=0.5, low=3,\n","                                    high=2, method='mad', center='median')\n","\n","            clipped_mask = ~np.isnan(clipped_flux)\n","\n","            clipped_time = time[clipped_mask]\n","            clipped_flux = clipped_flux[clipped_mask]\n","\n","            # see https://wotan.readthedocs.io/en/latest/Usage.html for other\n","            # possible options.\n","            flat_flux, trend_flux = wotan.flatten(\n","                clipped_time, clipped_flux, return_trend=True,\n","                method=dtrdict['method'],\n","                break_tolerance=dtrdict['break_tolerance'],\n","                window_length=dtrdict['window_length'],\n","                cval=dtrdict['cval']\n","            )\n","\n","            flat_mean, flat_sigma = (np.nanmean(flat_flux), np.nanstd(flat_flux))\n","\n","            #_, *bounds = sigma_clip(flat_flux, sigma_lower=10, sigma_upper=1, maxiters=1, masked=False, return_bounds=True) # okay flex LOL\n","\n","            bounds = [flat_mean-sigma_bounds[0]*flat_sigma, flat_mean+sigma_bounds[1]*flat_sigma] # save these bounds to log file? \n","\n","            flat_mask = np.logical_and(flat_flux<bounds[1], flat_flux>bounds[0])\n","\n","            flat_time = clipped_time[flat_mask]\n","            flat_flux = flat_flux[flat_mask]\n","\n","            lc_times = np.concatenate((lc_times, flat_time))\n","            lc_fluxes = np.concatenate((lc_fluxes, flat_flux))\n","\n","            trend_times = np.concatenate((trend_times, clipped_time))\n","            trend_fluxes = np.concatenate((trend_fluxes, trend_flux))\n","\n","            raw_time = np.concatenate((raw_times, time))\n","            raw_fluxe = np.concatenate((raw_fluxes, flux))\n","             \n","        # remove stuff below \n","\n","        continue_lower_cut = True \n","        while continue_lower_cut: \n","            below_lower_cut = np.where(flux<(np.median(flux)-lower_sigma*np.std(flux)))[0]\n","            if len(below_lower_cut)>0: \n","                time = np.delete(time, below_lower_cut)\n","                flux = np.delete(flux, below_lower_cut)\n","\n","            else: \n","                continue_lower_cut=False\n","\n","        (cleaned_time_temp, cleaned_flux_temp), (_, _) = remove_flares(time, flux)\n","\n","        detrended_flux_temp, trend_flux_temp = wotan.flatten(\n","            cleaned_time_temp, cleaned_flux_temp, return_trend=True,\n","            method=dtrdict['method'],\n","            break_tolerance=dtrdict['break_tolerance'],\n","            window_length=dtrdict['window_length'],\n","            cval=dtrdict['cval']\n","        )\n","\n","        (cleaned_time_temp, detrended_flux_temp, trend_flux_temp), (_, _, _) = remove_flares(cleaned_time_temp, detrended_flux_temp, trend_flux_temp)\n","\n","        cleaned_time = np.concatenate((cleaned_time, cleaned_time_temp))\n","        detrended_flux = np.concatenate((detrended_flux, detrended_flux_temp))\n","        trend_time = np.concatenate((trend_time, cleaned_time_temp))\n","        trend_flux = np.concatenate((trend_flux, trend_flux_temp))\n","\n","    if outdir != 'none': \n","        if outdir[-1]!='/':\n","            outdir+='/'\n","        \n","        outfile = outdir+ticstr.replace(' ','_')+'.csv'\n","        \n","        cols = [cleaned_time, detrended_flux, trend_time, trend_flux, raw_time, raw_flux]\n","        cols = [pd.Series(i) for i in cols]\n","\n","        col_names = ['cleaned_time', 'detrended_flux', 'trend_time', 'trend_flux', 'raw_time', 'raw_flux']\n","    \n","        dictionary = {}\n","        for i in range(len(cols)):\n","            dictionary.update({col_names[i]:cols[i]})\n","\n","        out_df = pd.DataFrame(dictionary)\n","\n","        out_df.to_csv(outfile, index=False)\n","\n","        # cleaned as in flares have been removed \n","        # detrended has had flares removed and trend removed as well  \n","\n","    return (cleaned_time, detrended_flux), (cleaned_time, trend_flux), (raw_time, raw_flux) \n"],"metadata":{"id":"1nMaa_zAaQGa","executionInfo":{"status":"ok","timestamp":1654223921729,"user_tz":420,"elapsed":466,"user":{"displayName":"Veronica Diaz","userId":"12949637830139422484"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["def download_and_preprocess(\n","    ticstr: str = '',\n","    outdir: str = 'none', \n","    logdir: str = 'none', \n","    dtrdict: dict = {'method':'biweight',\n","                     'window_length':0.5,\n","                     'cval':5.0,\n","                     \"break_tolerance\":1.0}, \n","    sigma_bounds: list = [10, 2]\n","    ): \n","    \n","    '''\n","    Args: \n","        ticstr: e.g. 'TIC 441420236'\n","        outdir: dir to save light curve to. default is none\n","        logdir: dir to save log file to. default is none\n","        dtrdict: detrending dictionary \n","        sigma_bounds: bounds for sigma clipping \n","        \n","    Returns: \n","        lc_list: list of light curve ojects that have met all criteria, been removed of outliers, normalized, and flattened. \n","        trend_list: list of light curve objects with x = time, y = trend\n","        raw_list: list of the raw light curve objects \n","        data_found: if data was not found during download, returns tuple of None objects\n","    '''\n","\n","    import numpy as np\n","    import warnings \n","\n","    raw_list, data_found = download(ticstr=ticstr, logdir=logdir) \n","\n","    if data_found: \n","        stitched_lc, stitched_trend, stitched_raw = preprocess(raw_list=raw_list, ticstr=ticstr, outdir=outdir, dtrdict=dtrdict)\n","\n","    else: \n","        stitched_lc, stitched_trend, stitched_raw = (None, None, None)\n","    \n","    warnings.warn('need to FIX/reimplement LOGGING! and get rid of download returning stitched lc!')\n","\n","    return stitched_lc, stitched_trend, stitched_raw, data_found\n"],"metadata":{"id":"zcff6Y0MaEOL","executionInfo":{"status":"ok","timestamp":1654223921729,"user_tz":420,"elapsed":8,"user":{"displayName":"Veronica Diaz","userId":"12949637830139422484"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","execution_count":34,"metadata":{"id":"BDQch_fmYlJo","executionInfo":{"status":"ok","timestamp":1654223921730,"user_tz":420,"elapsed":8,"user":{"displayName":"Veronica Diaz","userId":"12949637830139422484"}}},"outputs":[],"source":["def filter_data(path:str):\n","    '''\n","    returns:\n","    filtered_df: the edited dataframe  \n","    '''\n","    import pandas as pd\n","    import numpy as np\n","    df = pd.read_csv(path)\n","\n","    # drop initials nans\n","    df = df.dropna(subset=['pl_orbper','sy_pnum','pl_controv_flag'])\n","\n","    df = df.where(np.logical_and(df['pl_orbper']>.5,df['pl_orbper']<15))\n","    df = df.where(df['pl_controv_flag']==0)\n","    df = df.where(df['sy_pnum']==1)\n","\n","    # filter out the data where the given fields did not meet the conditions\n","    filtered_df = df.dropna(subset=['pl_orbper','pl_controv_flag','sy_pnum'])\n","\n","    return filtered_df"]},{"cell_type":"code","source":["def bls_check():\n","    df = filter_data('/content/tess nasa exo archive confirmed.csv')\n","    list = df.sample(n=10)\n","\n","    for tid in list:\n","        print(tid[''])\n","        lc_list, trend_list, raw_list = download_and_preprocess(ticstr=tid)\n","        "],"metadata":{"id":"5cXjYd6lZUll","executionInfo":{"status":"ok","timestamp":1654224109261,"user_tz":420,"elapsed":11,"user":{"displayName":"Veronica Diaz","userId":"12949637830139422484"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["bls_check()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":376},"id":"Id01iN_7askb","executionInfo":{"status":"error","timestamp":1654224112129,"user_tz":420,"elapsed":725,"user":{"displayName":"Veronica Diaz","userId":"12949637830139422484"}},"outputId":"8c4d837a-4b71-42df-d744-b328a1397432"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["loc_rowid\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: WARNING: THIS SHOULD NO LONGER BE USED BY ITSELF! NEEDS TO BE FIXED...USE download_preprocess INSTEAD\n","/usr/local/lib/python3.7/dist-packages/lightkurve/search.py:415: LightkurveWarning: Cannot download from an empty search result.\n","  \"Cannot download from an empty search result.\", LightkurveWarning\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: UserWarning: need to FIX/reimplement LOGGING! and get rid of download returning stitched lc!\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-27afe9529709>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbls_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-39-a8df73da96ed>\u001b[0m in \u001b[0;36mbls_check\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mlc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrend_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_and_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticstr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"]}]}]}