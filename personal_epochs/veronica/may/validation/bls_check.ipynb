{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6211,"status":"ok","timestamp":1654223921268,"user":{"displayName":"Veronica Diaz","userId":"12949637830139422484"},"user_tz":420},"id":"AtXHCNq9bDgz","outputId":"ab170d41-35f6-4b9f-8681-178b1cdc2bac"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting lightkurve\n","  Using cached lightkurve-2.2.1-py3-none-any.whl (254 kB)\n","Collecting beautifulsoup4>=4.6.0\n","  Using cached beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n","Collecting fbpca>=1.0\n","  Using cached fbpca-1.0.tar.gz (11 kB)\n","Collecting oktopus>=0.1.2\n","  Using cached oktopus-0.1.2.tar.gz (10 kB)\n","Collecting patsy>=0.5.0\n","  Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB)\n","Requirement already satisfied: pandas>=1.1.4 in c:\\python310\\lib\\site-packages (from lightkurve) (1.4.2)\n","Collecting matplotlib>=3.1\n","  Using cached matplotlib-3.5.2-cp310-cp310-win_amd64.whl (7.2 MB)\n","Requirement already satisfied: requests>=2.22.0 in c:\\python310\\lib\\site-packages (from lightkurve) (2.27.1)\n","Collecting astroquery>=0.3.10\n","  Using cached astroquery-0.4.6-py3-none-any.whl (4.5 MB)\n","Collecting bokeh>=1.1\n","  Using cached bokeh-2.4.3-py3-none-any.whl (18.5 MB)\n","Requirement already satisfied: numpy>=1.18 in c:\\python310\\lib\\site-packages (from lightkurve) (1.22.0)\n","Requirement already satisfied: scipy>=1.7 in c:\\python310\\lib\\site-packages (from lightkurve) (1.8.1)\n","Collecting uncertainties>=3.1.4\n","  Using cached uncertainties-3.1.6-py2.py3-none-any.whl (98 kB)\n","Collecting tqdm>=4.25.0\n","  Using cached tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n","Requirement already satisfied: urllib3>=1.23 in c:\\python310\\lib\\site-packages (from lightkurve) (1.26.8)\n","Collecting scikit-learn>=0.24.0\n","  Using cached scikit_learn-1.1.1-cp310-cp310-win_amd64.whl (7.3 MB)\n","Collecting memoization>=0.3.1\n","  Using cached memoization-0.4.0.tar.gz (41 kB)\n","Requirement already satisfied: astropy>=5.0 in c:\\python310\\lib\\site-packages (from lightkurve) (5.1)\n","Requirement already satisfied: PyYAML>=3.13 in c:\\python310\\lib\\site-packages (from astropy>=5.0->lightkurve) (6.0)\n","Requirement already satisfied: packaging>=19.0 in c:\\python310\\lib\\site-packages (from astropy>=5.0->lightkurve) (21.3)\n","Requirement already satisfied: pyerfa>=2.0 in c:\\python310\\lib\\site-packages (from astropy>=5.0->lightkurve) (2.0.0.1)\n","Collecting html5lib>=0.999\n","  Using cached html5lib-1.1-py2.py3-none-any.whl (112 kB)\n","Collecting pyvo>=1.1\n","  Using cached pyvo-1.3-py3-none-any.whl (829 kB)\n","Collecting keyring>=4.0\n","  Using cached keyring-23.5.1-py3-none-any.whl (33 kB)\n","Requirement already satisfied: soupsieve>1.2 in c:\\python310\\lib\\site-packages (from beautifulsoup4>=4.6.0->lightkurve) (2.3.2.post1)\n","Collecting typing-extensions>=3.10.0\n","  Using cached typing_extensions-4.2.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: pillow>=7.1.0 in c:\\python310\\lib\\site-packages (from bokeh>=1.1->lightkurve) (9.1.1)\n","Collecting Jinja2>=2.9\n","  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n","Collecting tornado>=5.1\n","  Using cached tornado-6.1.tar.gz (497 kB)\n","Requirement already satisfied: webencodings in c:\\python310\\lib\\site-packages (from html5lib>=0.999->astroquery>=0.3.10->lightkurve) (0.5.1)\n","Requirement already satisfied: six>=1.9 in c:\\python310\\lib\\site-packages (from html5lib>=0.999->astroquery>=0.3.10->lightkurve) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\python310\\lib\\site-packages (from Jinja2>=2.9->bokeh>=1.1->lightkurve) (2.1.1)\n","Requirement already satisfied: importlib-metadata>=3.6 in c:\\python310\\lib\\site-packages (from keyring>=4.0->astroquery>=0.3.10->lightkurve) (4.11.4)\n","Requirement already satisfied: pywin32-ctypes!=0.1.0,!=0.1.1 in c:\\python310\\lib\\site-packages (from keyring>=4.0->astroquery>=0.3.10->lightkurve) (0.2.0)\n","Requirement already satisfied: zipp>=0.5 in c:\\python310\\lib\\site-packages (from importlib-metadata>=3.6->keyring>=4.0->astroquery>=0.3.10->lightkurve) (3.8.0)\n","Requirement already satisfied: fonttools>=4.22.0 in c:\\python310\\lib\\site-packages (from matplotlib>=3.1->lightkurve) (4.33.3)\n","Requirement already satisfied: pyparsing>=2.2.1 in c:\\python310\\lib\\site-packages (from matplotlib>=3.1->lightkurve) (3.0.6)\n","Collecting cycler>=0.10\n","  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n","Requirement already satisfied: python-dateutil>=2.7 in c:\\python310\\lib\\site-packages (from matplotlib>=3.1->lightkurve) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python310\\lib\\site-packages (from matplotlib>=3.1->lightkurve) (1.4.2)\n","Collecting autograd\n","  Using cached autograd-1.4-py3-none-any.whl (48 kB)\n","Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas>=1.1.4->lightkurve) (2022.1)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python310\\lib\\site-packages (from requests>=2.22.0->lightkurve) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests>=2.22.0->lightkurve) (2022.5.18.1)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests>=2.22.0->lightkurve) (3.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python310\\lib\\site-packages (from scikit-learn>=0.24.0->lightkurve) (3.1.0)\n","Requirement already satisfied: joblib>=1.0.0 in c:\\python310\\lib\\site-packages (from scikit-learn>=0.24.0->lightkurve) (1.1.0)\n","Collecting colorama\n","  Using cached colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: future in c:\\python310\\lib\\site-packages (from uncertainties>=3.1.4->lightkurve) (0.18.2)\n","Using legacy 'setup.py install' for fbpca, since package 'wheel' is not installed.\n","Using legacy 'setup.py install' for memoization, since package 'wheel' is not installed.\n","Using legacy 'setup.py install' for oktopus, since package 'wheel' is not installed.\n","Using legacy 'setup.py install' for tornado, since package 'wheel' is not installed.\n","Installing collected packages: typing-extensions, tornado, pyvo, keyring, Jinja2, html5lib, cycler, colorama, beautifulsoup4, autograd, uncertainties, tqdm, scikit-learn, patsy, oktopus, memoization, matplotlib, fbpca, bokeh, astroquery, lightkurve\n","    Running setup.py install for tornado: started\n","    Running setup.py install for tornado: finished with status 'done'\n"]},{"name":"stderr","output_type":"stream","text":["  WARNING: Failed to write executable - trying to use .deleteme logic\n","ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'C:\\\\Python310\\\\Scripts\\\\keyring.exe' -> 'C:\\\\Python310\\\\Scripts\\\\keyring.exe.deleteme'\n","\n","WARNING: You are using pip version 21.2.4; however, version 22.1.2 is available.\n","You should consider upgrading via the 'C:\\Python310\\python.exe -m pip install --upgrade pip' command.\n"]},{"name":"stdout","output_type":"stream","text":["Collecting wotan\n","  Using cached wotan-1.10-py3-none-any.whl (30 kB)\n","Requirement already satisfied: numpy in c:\\python310\\lib\\site-packages (from wotan) (1.22.0)\n","Requirement already satisfied: numba in c:\\python310\\lib\\site-packages (from wotan) (0.55.2)\n","Requirement already satisfied: scipy in c:\\python310\\lib\\site-packages (from wotan) (1.8.1)\n","Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in c:\\python310\\lib\\site-packages (from numba->wotan) (0.38.1)\n","Requirement already satisfied: setuptools in c:\\python310\\lib\\site-packages (from numba->wotan) (58.1.0)\n","Installing collected packages: wotan\n","Successfully installed wotan-1.10\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: You are using pip version 21.2.4; however, version 22.1.2 is available.\n","You should consider upgrading via the 'C:\\Python310\\python.exe -m pip install --upgrade pip' command.\n"]}],"source":["!pip install lightkurve\n","!pip install wotan"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1654223921268,"user":{"displayName":"Veronica Diaz","userId":"12949637830139422484"},"user_tz":420},"id":"8IRTeiySaRz2"},"outputs":[{"ename":"NameError","evalue":"name 'np' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25080/3885117171.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mremove_flares\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflux\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflux_err\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     ''' \n\u001b[0;32m      3\u001b[0m     \u001b[0mArgs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mideally\u001b[0m \u001b[0mPandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtime\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mflux\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mideally\u001b[0m \u001b[0mPandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mof\u001b[0m \u001b[0mflux\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"]}],"source":["def remove_flares(time, flux, flux_err=np.array([]), sigma:int=3): \n","    ''' \n","    Args:\n","        time: array-like (ideally Pandas.series) object of time values \n","        flux: array-like (ideally Pandas.series) object of flux values\n","        flux_err: optional array-like (ideally Pandas.series) object of flux err values\n","    Returns: \n","        two tuples (or three if an array was passed into flux_err); first tuple gives arrays that have been \"cleaned\" of flairs, and second gives the removed values. Array order in each tuple is time, flux, (flux_err)\n","    Additional Info: \n","        Idea was taken from https://iopscience.iop.org/article/10.3847/1538-3881/ab5d3a\n","    '''\n","    import pandas as pd\n","    import numpy as np\n","\n","\n","    if isinstance(time, pd.Series) and isinstance(flux, pd.Series): \n","        pass\n","    else: \n","        time = pd.Series(time)\n","        flux = pd.Series(flux)\n","        if len(flux_err)>0: \n","            flux_err = pd.Series(flux_err)\n","            removed_flux_err = np.array([])\n","\n","    removed_time, removed_flux = (np.array([]), np.array([]))\n","\n","    continue_global = True \n","    while continue_global:\n","        length = len(flux)\n","\n","        # # We use three copies and extract the flares from the middle to prevent edge effects --> BREDALLL! LOL\n","        three_flux = pd.concat((flux, flux, flux))\n","        global_rolling = three_flux.rolling(1024, center=False) \n","        global_medians = global_rolling.median()[length:2*length]\n","        global_stds = global_rolling.std()[length:2*length]\n","        cutoffs = global_medians+sigma*global_stds\n","\n","        remove_indices = []\n","        for i in flux.index: \n","            if flux[i]>cutoffs[i] and global_medians[i]!=np.nan: \n","                remove_indices.append(i)\n","\n","        if len(remove_indices)==0: \n","            continue_global = False \n","\n","        else:     \n","            removed_time = np.concatenate((removed_time, time[remove_indices])) \n","            removed_flux = np.concatenate((removed_flux, flux[remove_indices])) \n","\n","            time = time.drop(remove_indices)\n","            flux = flux.drop(remove_indices)\n","\n","            if len(flux_err)>0: \n","                removed_flux_err = np.concatenate((removed_flux_err, flux_err[remove_indices]))\n","                flux_err = flux_err.drop(remove_indices)\n","            \n","    continue_local = True \n","    while continue_local: \n","        local_rolling = flux.rolling(128, center=True) \n","        local_medians = local_rolling.median() \n","        local_stds = local_rolling.std() \n","        cutoffs = local_medians+sigma*local_stds \n","\n","        remove_indices = [] \n","        for i in flux.index: \n","            if flux[i]>cutoffs[i] and local_medians[i]!=np.nan: \n","                remove_indices.append(i) \n","\n","        if len(remove_indices)==0: \n","            continue_local = False \n","\n","        else:     \n","            removed_time = np.concatenate((removed_time, time[remove_indices]))\n","            removed_flux = np.concatenate((removed_flux, flux[remove_indices]))\n","\n","            time = time.drop(remove_indices)\n","            flux = flux.drop(remove_indices)\n","\n","            if len(flux_err)>0: \n","                removed_flux_err = np.concatenate((removed_flux_err, flux_err[remove_indices]))\n","                flux_err = flux_err.drop(remove_indices)\n","\n","    if len(flux_err)>0: \n","        return (time.to_numpy(), flux.to_numpy(), flux_err.to_numpy()), (removed_time, removed_flux, removed_flux_err)\n","    else: \n","        return (time.to_numpy(), flux.to_numpy()), (removed_time, removed_flux)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":466,"status":"ok","timestamp":1654223921729,"user":{"displayName":"Veronica Diaz","userId":"12949637830139422484"},"user_tz":420},"id":"1nMaa_zAaQGa"},"outputs":[],"source":["def download(\n","    ticstr: str, \n","    outdir: str = 'none', \n","    logdir: str = 'none'): \n","    \n","    ''' \n","    Args:\n","        outdir: directory where lightcurves will be saved. If not set, data will not be saved. \n","        ticstr: e.g., 'TIC 441420236'\n","        logdir: directory for log file\n","    Returns: \n","        raw_list: list of light curve ojects that mean criteria but have not been processed (i.e. not detrended, normalized, or sigma-clipped) \n","        data_found: boolean variable telling you if data were found or not \n","    '''\n","\n","    import numpy as np \n","    import lightkurve as lk \n","    import os \n","    import pickle \n","    import warnings\n","\n","    # get the light curve\n","    data_found = False\n","    \n","    warnings.warn('WARNING: THIS SHOULD NO LONGER BE USED BY ITSELF! NEEDS TO BE FIXED...USE download_preprocess INSTEAD')\n","\n","    '''\n","    lcset = lk.search_lightcurve(ticstr) # otherwise it'll fail for TIC IDs without 120 second cadence data.\n","    if len(lcset) > 0:\n","        lcc = lcset[(lcset.author=='SPOC') & (lcset.exptime.value==120)].download_all()\n","        data_found = True\n","    '''\n","\n","    lcc = lk.search_lightcurve(ticstr.replace('_', ' ')).download_all() # FIX THIS! \n","    if lcc != None: \n","        data_found = True \n","\n","    # select only the two-minute cadence SPOC-reduced data; convert to a list.\n","    # note that this conversion approach works for any LightCurveCollection\n","    # returned by lightkurve -- no need to hand-pick the right ones.  the exact\n","    # condition below says \"if the interval is between 119 and 121 seconds,\n","    # take it\".\n","\n","    if data_found: \n","        raw_list = [_l for _l in lcc\n","                if\n","                _l.meta['ORIGIN']=='NASA/Ames'\n","                and\n","                np.isclose(\n","                    120,\n","                    np.nanmedian(np.diff(_l.remove_outliers().time.value))*24*60*60,\n","                    atol=1\n","                )\n","        ]\n","\n","        raw_list = [_l for _l in raw_list if _l.meta['FLUX_ORIGIN']=='pdcsap_flux']\n","        \n","        if len(raw_list) == 0: \n","            data_found = False \n","\n","        if data_found: \n","            new_raw_list = []\n","\n","            for lc in raw_list: \n","                time = lc.time.value\n","                flux = lc.flux.value\n","\n","                nan_mask = np.isnan(flux)\n","\n","                time = time[~nan_mask]\n","                flux = np.array(flux[~nan_mask], dtype=float) \n","\n","                qual = lc.quality.value\n","\n","                # remove non-zero quality flags\n","                sel = (qual[~nan_mask] == 0)\n","\n","                time = time[sel]\n","                flux = flux[sel]\n","\n","                # normalize around 1\n","                flux /= np.nanmedian(flux)\n","\n","                new_raw_list.append({'time':time, 'flux':flux})\n","\n","            raw_list = new_raw_list \n","    \n","    if data_found: \n","        if outdir != 'none': \n","            joined = {'raw_list':raw_list}\n","            outfile = outdir + '/' + ticstr.replace(' ', '_') + '_raw_lc.pickle'\n","            \n","            with open(outfile, 'wb') as handle:\n","                pickle.dump(joined, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","            warnings.warn(\"FIX THIS TO CSV!!!\")\n","            warnings.warn('idea for the future: just only use download and preprocess together')\n","\n","    if not data_found: \n","        raw_list = None\n","\n","    '''\n","    if logdir != 'none': \n","            logfile = logdir + '/' + ticstr.replace(\" \",'_')+'_log.pickle'\n","\n","            logfile = convert_to_absolute_path(logfile)\n","            if os.path.exists(logfile): \n","                with open(logfile, 'rb') as f: \n","                    content = pickle.load(f)\n","                    content['sectors']=len(raw_list)\n","\n","            else: \n","                content = {'sectors':len(raw_list)}\n","\n","            with open(logfile, 'wb') as f: \n","                    pickle.dump(content, f, protocol=pickle.HIGHEST_PROTOCOL)\n","    '''\n","    return raw_list, data_found\n","\n","def preprocess(\n","    raw_list: list,\n","    ticstr: str = '',\n","    outdir: str = \"none\", \n","    dtrdict: dict = {'method':'biweight',\n","                     'window_length':0.5,\n","                     'cval':5.0,\n","                     \"break_tolerance\":1.0}, \n","    lower_sigma: int = 10\n","    ):\n","    \"\"\"\n","    Args:\n","        raw_list: raw list of light curves; see download() \n","        outdir: directory where lightcurves will be saved. If not set, data will not be saved.  --> FIX!\n","        ticstr: e.g., 'TIC 441420236'.  \n","        dtrdict: dictionary with keys \"window_length\", \"method\", \"cval\",\n","                 \"break_tolerance\", or anything else needed by wotan's `flatten`\n","                 call.  These are documented at\n","                 https://wotan.readthedocs.io/en/latest/Usage.html \n","        lower_sigma: sigma value for \"lower\" (non-Gunther) level sigma clip. Default is 10.  \n","    Returns: \n","        lc_list: list of light curve ojects that have met all criteria, been removed of outliers, normalized, and flattened. \n","        trend_list: list of light curve objects with x = time, y = trend\n","        raw_list: list of the raw light curve objects \n","    \"\"\"\n","\n","    import numpy as np \n","    import wotan \n","    import pandas as pd\n","    import warnings\n","\n","    cleaned_time = np.array([])\n","    detrended_flux = np.array([])\n","    trend_time = np.array([])\n","    trend_flux = np.array([])\n","    raw_time = np.array([])\n","    raw_flux = np.array([])\n","\n","    for lc in raw_list:\n","\n","        time = lc['time']\n","        flux = lc['flux']\n","\n","        raw_time = np.concatenate((raw_time, time))\n","        raw_flux = np.concatenate((raw_flux, flux))\n","\n","        def old_detrend_method(time, flux): \n","\n","            # remove outliers before local window detrending-- wotan does this before detrend, and sigma clip after detrend \n","            clipped_flux = wotan.slide_clip(time, flux, window_length=0.5, low=3,\n","                                    high=2, method='mad', center='median')\n","\n","            clipped_mask = ~np.isnan(clipped_flux)\n","\n","            clipped_time = time[clipped_mask]\n","            clipped_flux = clipped_flux[clipped_mask]\n","\n","            # see https://wotan.readthedocs.io/en/latest/Usage.html for other\n","            # possible options.\n","            flat_flux, trend_flux = wotan.flatten(\n","                clipped_time, clipped_flux, return_trend=True,\n","                method='biweight',\n","                break_tolerance=dtrdict['break_tolerance'],\n","                window_length=dtrdict['window_length'],\n","                cval=dtrdict['cval']\n","            )\n","\n","            flat_mean, flat_sigma = (np.nanmean(flat_flux), np.nanstd(flat_flux))\n","\n","            #_, *bounds = sigma_clip(flat_flux, sigma_lower=10, sigma_upper=1, maxiters=1, masked=False, return_bounds=True) # okay flex LOL\n","\n","            bounds = [flat_mean-sigma_bounds[0]*flat_sigma, flat_mean+sigma_bounds[1]*flat_sigma] # save these bounds to log file? \n","\n","            flat_mask = np.logical_and(flat_flux<bounds[1], flat_flux>bounds[0])\n","\n","            flat_time = clipped_time[flat_mask]\n","            flat_flux = flat_flux[flat_mask]\n","\n","            lc_times = np.concatenate((lc_times, flat_time))\n","            lc_fluxes = np.concatenate((lc_fluxes, flat_flux))\n","\n","            trend_times = np.concatenate((trend_times, clipped_time))\n","            trend_fluxes = np.concatenate((trend_fluxes, trend_flux))\n","\n","            raw_time = np.concatenate((raw_times, time))\n","            raw_fluxe = np.concatenate((raw_fluxes, flux))\n","             \n","        # remove stuff below \n","\n","        continue_lower_cut = True \n","        while continue_lower_cut: \n","            below_lower_cut = np.where(flux<(np.median(flux)-lower_sigma*np.std(flux)))[0]\n","            if len(below_lower_cut)>0: \n","                time = np.delete(time, below_lower_cut)\n","                flux = np.delete(flux, below_lower_cut)\n","\n","            else: \n","                continue_lower_cut=False\n","\n","        (cleaned_time_temp, cleaned_flux_temp), (_, _) = remove_flares(time, flux)\n","\n","        detrended_flux_temp, trend_flux_temp = wotan.flatten(\n","            cleaned_time_temp, cleaned_flux_temp, return_trend=True,\n","            method=dtrdict['method'],\n","            break_tolerance=dtrdict['break_tolerance'],\n","            window_length=dtrdict['window_length'],\n","            cval=dtrdict['cval']\n","        )\n","\n","        (cleaned_time_temp, detrended_flux_temp, trend_flux_temp), (_, _, _) = remove_flares(cleaned_time_temp, detrended_flux_temp, trend_flux_temp)\n","\n","        cleaned_time = np.concatenate((cleaned_time, cleaned_time_temp))\n","        detrended_flux = np.concatenate((detrended_flux, detrended_flux_temp))\n","        trend_time = np.concatenate((trend_time, cleaned_time_temp))\n","        trend_flux = np.concatenate((trend_flux, trend_flux_temp))\n","\n","    if outdir != 'none': \n","        if outdir[-1]!='/':\n","            outdir+='/'\n","        \n","        outfile = outdir+ticstr.replace(' ','_')+'.csv'\n","        \n","        cols = [cleaned_time, detrended_flux, trend_time, trend_flux, raw_time, raw_flux]\n","        cols = [pd.Series(i) for i in cols]\n","\n","        col_names = ['cleaned_time', 'detrended_flux', 'trend_time', 'trend_flux', 'raw_time', 'raw_flux']\n","    \n","        dictionary = {}\n","        for i in range(len(cols)):\n","            dictionary.update({col_names[i]:cols[i]})\n","\n","        out_df = pd.DataFrame(dictionary)\n","\n","        out_df.to_csv(outfile, index=False)\n","\n","        # cleaned as in flares have been removed \n","        # detrended has had flares removed and trend removed as well  \n","\n","    return (cleaned_time, detrended_flux), (cleaned_time, trend_flux), (raw_time, raw_flux) \n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1654223921729,"user":{"displayName":"Veronica Diaz","userId":"12949637830139422484"},"user_tz":420},"id":"zcff6Y0MaEOL"},"outputs":[],"source":["def download_and_preprocess(\n","    ticstr: str = '',\n","    outdir: str = 'none', \n","    logdir: str = 'none', \n","    dtrdict: dict = {'method':'biweight',\n","                     'window_length':0.5,\n","                     'cval':5.0,\n","                     \"break_tolerance\":1.0}, \n","    sigma_bounds: list = [10, 2]\n","    ): \n","    \n","    '''\n","    Args: \n","        ticstr: e.g. 'TIC 441420236'\n","        outdir: dir to save light curve to. default is none\n","        logdir: dir to save log file to. default is none\n","        dtrdict: detrending dictionary \n","        sigma_bounds: bounds for sigma clipping \n","        \n","    Returns: \n","        lc_list: list of light curve ojects that have met all criteria, been removed of outliers, normalized, and flattened. \n","        trend_list: list of light curve objects with x = time, y = trend\n","        raw_list: list of the raw light curve objects \n","        data_found: if data was not found during download, returns tuple of None objects\n","    '''\n","\n","    import numpy as np\n","    import warnings \n","\n","    raw_list, data_found = download(ticstr=ticstr, logdir=logdir) \n","\n","    if data_found: \n","        stitched_lc, stitched_trend, stitched_raw = preprocess(raw_list=raw_list, ticstr=ticstr, outdir=outdir, dtrdict=dtrdict)\n","\n","    else: \n","        stitched_lc, stitched_trend, stitched_raw = (None, None, None)\n","    \n","    warnings.warn('need to FIX/reimplement LOGGING! and get rid of download returning stitched lc!')\n","\n","    return stitched_lc, stitched_trend, stitched_raw, data_found\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1654223921730,"user":{"displayName":"Veronica Diaz","userId":"12949637830139422484"},"user_tz":420},"id":"BDQch_fmYlJo"},"outputs":[],"source":["def filter_data(path:str):\n","    '''\n","    returns:\n","    filtered_df: the edited dataframe  \n","    '''\n","    import pandas as pd\n","    import numpy as np\n","    df = pd.read_csv(path)\n","\n","    # drop initials nans\n","    df = df.dropna(subset=['pl_orbper','sy_pnum','pl_controv_flag'])\n","\n","    df = df.where(np.logical_and(df['pl_orbper']>.5,df['pl_orbper']<15))\n","    df = df.where(df['pl_controv_flag']==0)\n","    df = df.where(df['sy_pnum']==1)\n","\n","    # filter out the data where the given fields did not meet the conditions\n","    filtered_df = df.dropna(subset=['pl_orbper','pl_controv_flag','sy_pnum'])\n","\n","    return filtered_df"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1654224109261,"user":{"displayName":"Veronica Diaz","userId":"12949637830139422484"},"user_tz":420},"id":"5cXjYd6lZUll"},"outputs":[],"source":["def bls_check():\n","    df = filter_data('/content/tess nasa exo archive confirmed.csv')\n","    list = df.sample(n=10)\n","\n","    for tid in list:\n","        print(tid[''])\n","        lc_list, trend_list, raw_list = download_and_preprocess(ticstr=tid)\n","    best_params, results, bls_model, in_transit, stats = run_bls(time, flux)\n","        "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":376},"executionInfo":{"elapsed":725,"status":"error","timestamp":1654224112129,"user":{"displayName":"Veronica Diaz","userId":"12949637830139422484"},"user_tz":420},"id":"Id01iN_7askb","outputId":"8c4d837a-4b71-42df-d744-b328a1397432"},"outputs":[{"ename":"NameError","evalue":"name 'lc_list' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20960/4124379960.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlc_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mNameError\u001b[0m: name 'lc_list' is not defined"]}],"source":["lc_list[0]"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPCFCXq+qIy6yYYDW/fzcch","name":"bls_check.ipynb","provenance":[]},"interpreter":{"hash":"483cc6728aea19aea79f46d08e82e5df36712545a012895e9b7c5b35b9f2510c"},"kernelspec":{"display_name":"Python 3.7.9 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"nbformat":4,"nbformat_minor":0}
